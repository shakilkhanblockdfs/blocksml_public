{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Linear Regression**\n",
    "Linear Regression is a supervised learning algorithm used to predict a continuous target variable based on one or more input features. It assumes a linear relationship between the input features and the target variable. The goal is to fit a line (or hyperplane in higher dimensions) that minimizes the difference between the predicted and actual values.\n",
    "\n",
    "When to Use:\n",
    "When you want to model the relationship between a continuous dependent variable and one or more independent variables.\n",
    "When you believe the relationship between variables is approximately linear.\n",
    "Example Problem:\n",
    "Predicting house prices based on features like square footage, number of bedrooms, and age of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Creating a sample dataset\n",
    "# Features (X) - size of house in square feet\n",
    "# Target (y) - price of the house in $1000s\n",
    "X = np.array([1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400]).reshape(-1, 1)\n",
    "y = np.array([300, 320, 340, 360, 380, 400, 420, 440, 460, 480])\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Fitting the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Visualizing the regression line\n",
    "plt.scatter(X, y, color='blue')  # Actual data points\n",
    "plt.plot(X, lr_model.predict(X), color='red')  # Regression line\n",
    "plt.title('House Prices vs. Size (Linear Regression)')\n",
    "plt.xlabel('Size of house (square feet)')\n",
    "plt.ylabel('Price ($1000)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Decision Tree**\n",
    "A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. The algorithm splits the dataset into subsets based on the value of the input features, recursively creating a tree of decisions. Each internal node represents a \"test\" on a feature, each branch represents the outcome of the test, and each leaf node represents a class or value.\n",
    "\n",
    "When to Use:\n",
    "When you need a model that's interpretable.\n",
    "When you have both categorical and numerical features.\n",
    "When the dataset contains non-linear relationships.\n",
    "Example Problem:\n",
    "Classifying whether a person will buy a product based on features like age, income, and product category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris dataset (a common classification dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Visualizing the decision tree (you need graphviz installed to view it as a visual tree)\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "tree.plot_tree(dt_model, filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Random Forest**\n",
    "Random Forest is an ensemble learning algorithm that creates multiple decision trees (using subsets of data and features) and combines their predictions to improve accuracy and reduce overfitting. It works for both classification and regression tasks.\n",
    "\n",
    "When to Use:\n",
    "When you want a more robust and less prone to overfitting version of a decision tree.\n",
    "When you have large datasets with a lot of noise or missing data.\n",
    "Example Problem:\n",
    "Predicting customer churn in a telecom company based on usage patterns, customer complaints, and subscription history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset again\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. AdaBoost**\n",
    "AdaBoost (Adaptive Boosting) is a boosting algorithm that combines multiple weak learners (usually shallow decision trees) to create a stronger model. It assigns more weight to misclassified data points at each iteration, focusing the next model on harder-to-classify instances.\n",
    "\n",
    "When to Use:\n",
    "When you have weak learners and want to improve their performance.\n",
    "When you have imbalanced data or noisy data.\n",
    "Example Problem:\n",
    "Classifying whether a customer will renew their subscription based on usage and complaints, where most customers easily renew, but some are difficult to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the AdaBoost model\n",
    "ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = ada_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Gradient Boosting**\n",
    "Gradient Boosting is another boosting technique where new models are added to correct the errors made by existing models. Unlike AdaBoost, which assigns weights to instances, Gradient Boosting optimizes based on the gradient of the loss function, adding models that reduce the overall error in a stepwise manner.\n",
    "\n",
    "When to Use:\n",
    "When you want a highly accurate model, especially for complex datasets with non-linear relationships.\n",
    "When you are okay with slightly longer training times.\n",
    "Example Problem:\n",
    "Predicting credit card fraud based on spending patterns, location, and transaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Logistic Regression**\n",
    "Logistic Regression is a classification algorithm used when the dependent variable is categorical. It estimates the probability that a given input belongs to a particular category, making it suitable for binary or multinomial classification.\n",
    "\n",
    "When to Use:\n",
    "When you have a binary or categorical target variable.\n",
    "When you need a simple and interpretable model for classification.\n",
    "Example Problem:\n",
    "Classifying whether an email is spam or not based on the presence of certain words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset for binary classification (only two classes)\n",
    "X = iris.data[:100]  # Only two classes\n",
    "y = iris.target[:100]\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fitting the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Support Vector Machine (SVM)**\n",
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It finds the hyperplane that best separates the data points of different classes, maximizing the margin between them.\n",
    "\n",
    "When to Use:\n",
    "When you have high-dimensional data.\n",
    "When you need a powerful and flexible classification model that can handle non-linear data through kernel tricks.\n",
    "Example Problem:\n",
    "Classifying types of cancer based on the results of multiple medical tests, where the data might not be linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the Support Vector Classifier model\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fitting the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. k-Nearest Neighbors (KNN)**\n",
    "k-Nearest Neighbors is a supervised learning algorithm used for both classification and regression tasks. The algorithm classifies a data point based on the majority class of its k-nearest neighbors in the feature space.\n",
    "\n",
    "When to Use:\n",
    "When you have small to medium-sized datasets.\n",
    "When you need a simple, instance-based learning model.\n",
    "It is sensitive to feature scaling, so it works better with normalized data.\n",
    "Example Problem:\n",
    "Classifying whether a person will purchase a product based on their demographic information by comparing them to similar past customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fitting the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. k-Means (Clustering)**\n",
    "k-Means is an unsupervised learning algorithm used for clustering data. It partitions data into k clusters based on feature similarity, where each data point belongs to the cluster with the nearest mean.\n",
    "\n",
    "When to Use:\n",
    "When you need to perform clustering to group similar data points together.\n",
    "When you have unlabelled data and want to find underlying patterns.\n",
    "Example Problem:\n",
    "Grouping customers based on their purchasing habits into clusters for targeted marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a sample dataset (2D points)\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7], [8, 8], [10, 12], [11, 13], [12, 14]])\n",
    "\n",
    "# Creating the k-Means model\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# Predicting the cluster for each point\n",
    "y_kmeans = kmeans_model.predict(X)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
    "plt.scatter(kmeans_model.cluster_centers_[:, 0], kmeans_model.cluster_centers_[:, 1], s=300, c='red')\n",
    "plt.title('k-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Collaborative Filtering (Recommender Systems)**\n",
    "Collaborative Filtering is a technique used in recommendation systems where predictions about a user's interests are made by collecting preferences or ratings from multiple users.\n",
    "\n",
    "When to Use:\n",
    "When you want to build a recommendation system based on user-item interactions.\n",
    "When you have user behavior data like ratings, purchases, or clicks.\n",
    "Example Problem:\n",
    "Recommending movies to users based on their viewing history and the preferences of other similar users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Example user-item interaction matrix (ratings)\n",
    "user_item_matrix = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [0, 0, 5, 4],\n",
    "    [0, 0, 5, 4],\n",
    "])\n",
    "\n",
    "# Applying SVD (Singular Value Decomposition)\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "user_factors = svd.fit_transform(user_item_matrix)\n",
    "item_factors = svd.components_\n",
    "\n",
    "# Reconstructing the user-item matrix\n",
    "reconstructed_matrix = np.dot(user_factors, item_factors)\n",
    "\n",
    "# Output reconstructed matrix (predicted ratings)\n",
    "print(\"Predicted User-Item Matrix:\")\n",
    "print(reconstructed_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
